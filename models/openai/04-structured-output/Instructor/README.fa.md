# مربی: خروجی های ساخت یافته LLM

[مربی] (https://github.com/jxnl/instructor) یک کتابخانه پایتون است که کار با خروجی های ساختاری از مدلهای زبان بزرگ (LLMS) کار را آسان می کند.ساخته شده در بالای [pydantic] (https://docs.pydantic.dev/latest/) ، یک API ساده ، شفاف و کاربر پسند را برای مدیریت اعتبار سنجی ، احیای مجدد و پاسخ های جریان فراهم می کند.این کتابخانه عملکرد فراخوانی ، تماس ابزار و حالت های نمونه برداری محدود مانند Mode JSON را برای دریافت خروجی ساختاری بر اساس طرح های پیودانتیک اعمال می کند.می توانید نمونه های بیشتری را در [کتاب آشپزی] (https://python.useinstructor.com/examples/) پیدا کنید و توضیحات مربوط به تمام مفاهیم تحت پوشش در کتابخانه [در اینجا ذکر شده است] (https://python.useinstructor.com/concepts/types/).

## ویژگی های کلیدی

- ** مدل های پاسخ **: برای تعریف ساختار خروجی های LLM خود ، مدل های Pydantic را مشخص کنید
- ** مدیریت مجدد **: تعداد تلاش های آزمایش مجدد برای درخواست های خود را پیکربندی کنید
- ** اعتبار سنجی **: اطمینان حاصل کنید که پاسخ های LLM مطابق با انتظارات خود با اعتبار سنجی Pydantic است
- ** پشتیبانی جریان **: با لیست ها و پاسخ های جزئی کار کنید
- ** Backends انعطاف پذیر **: با ارائه دهندگان مختلف LLM فراتر از OpenAi ادغام شوید

## نصب

مربی را با یک دستور واحد نصب کنید:

`` `bash
PIP Install -u مربی
`` `

حال ، بیایید مربی را در عمل با یک مثال ساده ببینیم:

`` `پایتون
مربی واردات
از Pydantic Import Basemodel
از Openai واردات Openai


# ساختار خروجی مورد نظر خود را تعریف کنید
کلاس UserInfo (Basemodel):
نام: خیابان
سن: int


# مشتری OpenAI را وصله کنید
client = مربی. from_openai (OpenAi ())

# داده های ساختاری را از زبان طبیعی استخراج کنید
user_info = client.chat.completions.create (
model = "GPT-3.5-turbo" ،
پاسخ_مودل = userInfo ،
پیام ها = [{"نقش": "کاربر" ، "محتوا": "جان دوی 30 ساله است."}]
)

چاپ (user_info.name)
#> جان دوه
چاپ (user_info.age)
#> 30
`` `


## استفاده از مربی برای اعتبار سنجی خروجی در برنامه های LLM

مربی ابزاری قدرتمند است که به شما امکان می دهد تا خروجی مدل های زبان (LLMS) را در برنامه های خود تأیید کنید.با تعریف یک ساختار خروجی مورد نظر با استفاده از مدل های Pydantic و تعیین قوانین اعتبار سنجی ، می توانید اطمینان حاصل کنید که پاسخ های تولید شده نیازهای شما را برآورده می کنند.اگر یک پرس و جو از بین برود ، می توانید به آن دستور دهید [به طور خودکار دوباره امتحان کنید] (https://python.useinstructor.com/concepts/retrying/).

### اعتبار سنجی دسته های enum

یک مورد استفاده متداول ، اعتبارسنجی دسته پاسخ با استفاده از [شمارش (enum)] (https://python.useinstructor.com/examples/classification/#defining-the-structures) است.در اینجا یک مثال آورده شده است:

`` `پایتون
طبقه بندی کلاس (Str ، enum):
"" "شمارش دسته ها برای بلیط های ورودی." ""

عمومی = "عمومی"
سفارش = "سفارش"
صورتحساب = "صورتحساب"

پاسخ کلاس (Basemodel):
محتوا: str = فیلد (توضیحات = "پاسخ شما که به مشتری ارسال می کنیم.")
رده: TicketCategory
اعتماد به نفس: float = زمینه (
ge = 0 ، le = 1 ، توضیحات = "اعتماد به نفس در پیش بینی دسته".
)
`` `

در این کد ، ما یک enum «TicketCategor» را تعریف می کنیم که نشانگر دسته های معتبر برای بلیط های ورودی است.سپس ما یک مدل "پاسخ" را با استفاده از Pydantic ایجاد می کنیم ، که شامل یک طبقه "دسته" از نوع "TicketCategor" است.

هنگامی که ما با استفاده از مربی درخواست LLM می کنیم ، می توانیم پارامتر `Response_Model` را برای تأیید پاسخ تولید شده در برابر مدل" پاسخ "مشخص کنیم:

`` `پایتون
پاسخ = client.chat.completions.create (
model = "GPT-3.5-turbo" ،
پاسخ_مودل = پاسخ ،
max_retries = 1 ،
پیام = [
{
"نقش": "سیستم" ،
"محتوا": "شما یک دستیار مفید مراقبت از مشتری هستید که می تواند پیام های دریافتی را طبقه بندی کرده و پاسخی ایجاد کند. دسته را روی" موز "قرار دهید."
} ،
{"نقش": "کاربر" ، "محتوا": پرس و جو} ،
]] ،
)
`` `

اگر پاسخ تولید شده حاوی یک دسته معتبر از enum «TicketCategor» باشد ، مربی خطای اعتبار سنجی را مطرح می کند.می توانید پارامتر "MAX_RETRIES" را مشخص کنید تا به طور خودکار درخواست را امتحان کنید و LLM را برای تولید یک اعتبار معتبر افزایش دهیدonse

### اعتبار سنجی اعتماد به نفس

یکی دیگر از سناریوی اعتبار سنجی مشترک این است که نمرات اعتماد به نفس تولید شده [در یک محدوده خاص قرار می گیرد] (https://docs.pydantic.dev/latest/concepts/fields/#numeric-constraints).در اینجا یک مثال آورده شده است:

`` `پایتون
پاسخ کلاس (Basemodel):
محتوا: str = فیلد (توضیحات = "پاسخ شما که به مشتری ارسال می کنیم.")
رده: TicketCategory
اعتماد به نفس: float = زمینه (
ge = 0 ، le = 1 ، توضیحات = "اعتماد به نفس در پیش بینی دسته".
)
`` `

در مدل "پاسخ" ، ما زمینه "اعتماد به نفس" را به عنوان شناور تعریف می کنیم و از اعتبار سنجی "Ge" (بزرگتر از یا مساوی) و "LE" (کمتر از یا مساوی) استفاده می کنیم تا نمره اعتماد به نفس را به دامنه 0 تا 1 محدود کنیم.

هنگام درخواست به LLM ، می توانیم دامنه مورد نظر را در پیام سیستم مشخص کنیم:

`` `پایتون
پاسخ = client.chat.completions.create (
model = "GPT-3.5-turbo" ،
پاسخ_مودل = پاسخ ،
max_retries = 3 ،
پیام = [
{
"نقش": "سیستم" ،
"محتوا": "شما یک دستیار مفید مراقبت از مشتری هستید که می تواند پیام های دریافتی را طبقه بندی کرده و پاسخی ایجاد کند. اعتماد به نفس بین 1-100 را تنظیم کنید."
} ،
{"نقش": "کاربر" ، "محتوا": پرس و جو} ،
]] ،
)
`` `

اگر نمره اعتماد به نفس تولید شده در محدوده مشخص قرار نگیرد ، مربی خطای اعتبار سنجی را مطرح می کند.با تنظیم `max_retries = 3` ، مربی به طور خودکار [دوباره سعی می کند] (https://python.useinstructor.com/concepts/retrying/) درخواست حداکثر سه بار ، و باعث می شود LLM نمره اطمینان معتبر ایجاد کند.


## فیلتر کردن محتوا

علاوه بر اعتبار سنجی دسته ها و نمرات اعتماد به نفس ، مربی همچنین یک ویژگی قدرتمند برای فیلتر محتوا و [تصحیح خود] (https://python.useinstructor.com/examples/self_critique/) با استفاده از `llm_validator` فراهم می کند.این به شما امکان می دهد اطمینان حاصل کنید که پاسخ های تولید شده به دستورالعمل ها یا قوانین خاص ، مانند اجتناب از محتوا که می تواند به اعتبار شرکت آسیب برساند ، رعایت کنید.آنها همچنین راهی را برای [https://python.useinstructor.com/examples/moderation/) با استفاده از نقطه پایانی OpenAI برای بررسی انطباق محتوا با سیاست های استفاده OpenAI ارائه می دهند.

در اینجا مثالی از نحوه استفاده از `llm_validator 'برای فیلتر محتوا آورده شده است:

`` `پایتون
از Pydantic Import Basemodel ، Forevalidator
از typing_extensions واردات حاشیه نویسی
از مربی واردات llm_validator

Class ValitedReply (Basemodel):
محتوا: حاشیه نویسی [
خیابان ،
قبل از validator (
LLM_Validator (
بیانیه = "هرگز چیزهایی را نگو که به اعتبار شرکت آسیب برساند."
مشتری = مشتری ،
Allow_Override = درست ،
)
)
]
`` `

در این کد ، ما یک مدل `alificatedReply" را با استفاده از Pydantic تعریف می کنیم.قسمت "محتوا" با "حاشیه نویسی" و "قبل از validator" حاشیه نویسی شده است تا یک اعتبار سنج سفارشی را با آن مرتبط کند.

عملکرد `LLM_Validator 'به عنوان اعتبار سنج استفاده می شود و چندین پارامتر طول می کشد:
- "بیانیه": رشته ای که نشان دهنده قانون اعتبار سنجی است که پاسخ تولید شده باید دنبال شود.در این حالت ، بیان می شود که پاسخ هرگز نباید حاوی محتوایی باشد که می تواند به اعتبار شرکت آسیب برساند.
- "مشتری": نمونه مشتری OpenAI برای برقراری تماس های API استفاده می شود.
- `Allow_Override": یک پرچم بولی که نشان می دهد آیا اعتبارسنجی در صورت لزوم می تواند نادیده گرفته شود یا خیر.

هنگام درخواست به LLM ، می توانیم از مدل `alificatedReply" به عنوان "پاسخ_مودل" استفاده کنیم:

`` `پایتون
سعی کنید:
پاسخ = client.chat.completions.create (
model = "GPT-3.5-turbo" ،
پاسخ_مودل = اعتبار سنجی ،
max_retries = 1 ،
پیام = [
{
"نقش": "سیستم" ،
"محتوا": "شما یک دستیار مفید مراقبت از مشتری هستید که می تواند پیام های دریافتی را طبقه بندی کرده و پاسخی ایجاد کند."
} ،
{"نقش": "کاربر" ، "محتوا": پرس و جو} ،
]] ،
)
به جز استثنا به عنوان E:
چاپ (E)
`` `

اگر پاسخ تولید شده حاوی محتوایی باشد که قانون اعتبار سنجی مشخص شده را نقض می کند ، `llm_validatیا با یک پیام خطای مفید ، `alificationError" را مطرح می کند.این پیام خطا را می توان گرفت و برای فوریت LLM برای تصحیح خود و ایجاد پاسخی که به قانون اعتبار سنجی پایبند است ، استفاده می شود.

با ترکیب فیلتر محتوا با استفاده از "LLM_Validator" ، می توانید اطمینان حاصل کنید که پاسخ های تولید شده دستورالعمل های محتوای خاص را رعایت کرده و از مشکلات احتمالی که می تواند به اعتبار شرکت آسیب برساند ، جلوگیری می کند.

### درک نحو

برای استفاده از `llm_validator 'برای فیلتر محتوا ، درک نحو و مؤلفه های موجود مهم است:

1. `Annotated`: این یک نوع اشاره از ماژول" Typing_extensions "است که اجازه می دهد تا ابرداده یا اطلاعات اضافی را به یک نوع متصل کنید.در این کد ، از آن برای مرتبط کردن "قبل از validator" با قسمت "محتوای" مدل `alificatedReply" استفاده می شود.

2. `قبل از Validator": این یک اعتبار سنج Pydantic است که قبل از تأیید مقدار فیلد در برابر نوع قسمت و سایر محدودیت ها اعمال می شود.این امکان را برای انجام منطق اعتبار سنجی سفارشی بر روی مقدار فیلد قبل از اعتبار سنجی استاندارد Pydantic فراهم می کند.در این کد ، از آن برای بسته بندی عملکرد "LLM_Validator" استفاده می شود.

3. `llm_validator`: این یک تابع اعتبار سنجی سفارشی است که توسط کتابخانه" مربی "ارائه شده است.این بیانیه اعتبار سنجی ، نمونه مشتری OpenAI و پرچم `level_override` را به عنوان پارامترها می گیرد.در داخل از API OpenAI برای تأیید پاسخ تولید شده در برابر بیانیه اعتبار سنجی ارائه شده استفاده می کند.در صورت عدم موفقیت اعتبار ، با یک پیام خطای مفید ، «اعتبارسنجی شرکت را ایجاد می کند.

با ترکیب این مؤلفه ها ، می توانید به طور مؤثر محتوای پاسخ های تولید شده را فیلتر کرده و اطمینان حاصل کنید که آنها دستورالعمل ها یا قوانین خاصی را رعایت می کنند.

## نتیجه گیری

مربی فرآیند اعتبار سنجی و فیلتر کردن خروجی های LLM را با استفاده از مدلهای Pydantic ، قوانین اعتبار سنجی و عملکرد "LLM_Validator" ساده می کند.با تعریف ساختار خروجی مورد نظر ، مشخص کردن محدودیت های اعتبار سنجی و ترکیب فیلتر محتوا ، می توانید اطمینان حاصل کنید که پاسخ های تولید شده نیازهای شما را برآورده می کنند و به دستورالعمل های خاص پایبند هستند.

این ترکیب قدرتمند از ویژگی ها باعث افزایش قابلیت اطمینان ، دقت و مناسب بودن پاسخ های ایجاد شده توسط برنامه های LLM شما می شود و در نهایت باعث افزایش تجربه کاربر و محافظت از اعتبار شرکت شما می شود.