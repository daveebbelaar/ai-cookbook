# خروجی ساختاری در برنامه های LLM

هنگام کار با API OpenAI به طور مستقیم ، شما دو گزینه اصلی برای به دست آوردن پاسخ های خروجی ساختاری از مدل های GPT دارید: حالت JSON و تماس با عملکرد.در حالی که هر دو ابزار قدرتمند هستند ، اما محدودیت های خود را نیز دارند.درک اینکه چه زمانی از هر کدام استفاده می شود می تواند گردش کار شما را تقویت کرده و کنترل بیشتری بر روی خروجی به شما بدهد.پس از یادگیری در مورد این دو روش ، ما به [مربی] (https://github.com/daveebbelaar/python-openai-tutorial/tree/04٪20structured٪20output/instructor) شیرجه می شویم تا کنترل بیشتری از مدل های OpenAi داشته باشیم.مربی در این بزرگ پوشیده شده بود ["Pydantic همه چیز شما نیاز دارید"] (https://www.youtube.com/watch؟v=yj-wsrjwrrc) صحبت های جیسون لیو.

## چرا از خروجی JSON استفاده می کنید؟

استفاده از خروجی JSON در برنامه های LLM شما کنترل و اعتبار بیشتری را در پاسخ های تولید شده فراهم می کند.این تضمین می کند که خروجی همیشه یک رشته معتبر JSON است و باعث می شود تجزیه و پردازش داده ها در برنامه شما آسانتر شود.

## حالت json

در [حالت JSON] (https://platform.openai.com/docs/guides/text-generation/json-mode) ، این مدل خروجی ها را به طور انحصاری به عنوان رشته های معتبر JSON تولید می کند.با این حال ، شما باید صریحاً ساختار JSON مورد نظر را در فوری سیستم مشخص کنید تا مدل را به سمت قالب مورد انتظار راهنمایی کنید.

در اینجا مثالی از استفاده از حالت JSON آورده شده است:

`` `پایتون
query = "سلام ، من در مورد صورتحساب خود سوالی دارم. آیا می توانید به من کمک کنید؟"

پیام = [
{
"نقش": "سیستم" ،
"محتوا": "" "
شما یک دستیار مفید مراقبت از مشتری هستید که می تواند پیام های دریافتی را طبقه بندی کرده و پاسخی ایجاد کند.
همیشه در فرمت JSON زیر پاسخ دهید: {"محتوا": <Pensponse> ، "دسته": <طبقه بندی>
دسته های موجود: "عمومی" ، "سفارش" ، "صورتحساب"
"" "،
} ،
{
"نقش": "کاربر" ،
"محتوا": پرس و جو ،
} ،
]

پاسخ = client.chat.completions.create (
model = "GPT-3.5-turbo" ،
پیام = پیام ،
پاسخ_فورمات = {"نوع": "json_object"} ،
)
پیام = پاسخ. انتخاب [0] .Message.Content

تایپ (پیام) # str

message_json = json.loads (پیام)
تایپ کنید (پیام_جسون) # دیکته
`` `

توجه به این نکته حائز اهمیت است که OpenAI تضمین نمی کند که متن خروجی فرمت JSON مشخص شده شما را داشته باشد.این فقط تضمین می کند که خروجی یک رشته معتبر خواهد بود که می تواند برای JSON تجزیه شود.

### مرجع API

- `Response_Format`: یک شیء را مشخص می کند که مدل باید از آن خارج شود.سازگار با GPT-4 توربو و تمام مدل های GPT-5.5 توربو جدیدتر از GPT-3.5-Turbo-1106.تنظیم روی `{" نوع ":" json_object "}` حالت JSON را فعال می کند ، که پیامی را که مدل ایجاد می کند تضمین می کند JSON معتبر است.

مهم: هنگام استفاده از حالت JSON ، شما همچنین باید به مدل دستور دهید که JSON خود را از طریق یک سیستم یا پیام کاربر تولید کند.بدون این ، مدل ممکن است یک جریان بی پایان از فضای سفید ایجاد کند تا اینکه نسل به حد توکن برسد و در نتیجه درخواست طولانی و به ظاهر "گیر" ایجاد شود.همچنین توجه داشته باشید که اگر "Finish_Reason =" طول "، محتوای پیام تا حدی قطع شود ، که نشان می دهد نسل بیش از" max_tokens "یا مکالمه از طول متن حداکثر فراتر رفته است.

## تماس عملکرد

[فراخوانی عملکرد] (https://platform.openai.com/docs/guides/function-calling) به شما امکان می دهد لیستی از توابع را که مدل می تواند با آن تماس بگیرد ، تهیه کنید.می توانید نام عملکرد ، توضیحات و پارامترها ، از جمله انواع آنها و زمینه های مورد نیاز را مشخص کنید.می توانید نمونه های بیشتری را در این [Cookbook] (https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models) پیدا کنید.

در اینجا مثالی از استفاده از عملکرد فراخوانی آورده شده است:

`` `پایتون
query = "سلام ، من در مورد صورتحساب خود سوالی دارم. آیا می توانید به من کمک کنید؟"

function_name = "گپ"

ابزار = [
{
"نوع": "عملکرد" ،
"عملکرد": {
"نام": function_name ،
"توضیحات": F "عملکرد برای پاسخ به پرس و جو مشتری."
"پارامترها": {
"نوع": "شی" ،
"خواص": {
"محتوا": {"نوع": "رشته" ،
"توضیحات": "پاسخ شما که به مشتری می فرستیم." ،
} ،
"دسته": {
"نوع": "رشته" ،
"enum": ["عمومی" ، "سفارش" ، "صورتحساب"] ،
"توضیحات": "دسته بلیط." ،
} ،
} ،
"مورد نیاز": ["محتوا" ، "دسته"] ،
} ،
} ،
}
]

پیام = [
{
"نقش": "سیستم" ،
"محتوا": "شما یک دستیار مفید مراقبت از مشتری هستید که می تواند پیام های دریافتی را طبقه بندی کرده و پاسخی ایجاد کند."
} ،
{
"نقش": "کاربر" ،
"محتوا": پرس و جو ،
} ،
]

پاسخ = client.chat.completions.create (
model = "GPT-3.5-turbo" ،
پیام = پیام ،
ابزار = ابزار ،
tool_choice = {"نوع": "تابع" ، "عملکرد": {"نام": function_name}} ،
)

tool_call = پاسخ. انتخاب [0] .message.tool_calls [0]
(Tool_call) # chatcompletionmessagetoolcall را تایپ کنید

function_args = json.loads (tool_call.function.arguments)
Type (function_args) # دیکته
`` `

### مرجع API

- `Tools`: لیستی از ابزارهایی که ممکن است مدل با آن تماس بگیرد.در حال حاضر ، فقط توابع به عنوان ابزاری پشتیبانی می شوند.از این استفاده کنید تا لیستی از کارکردهایی که مدل ممکن است ورودی های JSON را برای آن ایجاد کند ، استفاده کنید.حداکثر 128 کارکرد پشتیبانی می شود.

- `tool_choice`: کنترل هایی که (در صورت وجود) توسط مدل نامیده می شود.`هیچ یک به این معنی است که مدل هیچ ابزاری را صدا نمی کند و در عوض پیامی ایجاد می کند.`auto` به این معنی است که مدل می تواند بین تولید پیام یا تماس با یک یا چند ابزار انتخاب کند.`مورد نیاز" به این معنی است که مدل باید با یک یا چند ابزار تماس بگیرد.مشخص کردن یک ابزار خاص از طریق `{" نوع ":" عملکرد "،" عملکرد ": {" نام ":" my_function "}}` مدل را مجبور می کند تا آن ابزار را فراخوانی کند."هیچ یک" پیش فرض نیست که هیچ ابزاری وجود نداشته باشد.اگر ابزارها وجود داشته باشند ، `auto` پیش فرض است.

## چه زمانی از هر روش استفاده می شود

1. ** فراخوانی عملکرد **: اگر مورد استفاده شما برای استفاده از تماس با عملکرد قاب می شود ، توصیه می شود از آن استفاده کنید.OpenAI به طور خودکار سریع شما را با توجه به توابع مشخص شده بهینه می کند و مدل های زبان با این فرمت سریع آموزش داده می شوند.این احتمال پاسخ های بهتر را افزایش داده و فرکانس توهم را کاهش می دهد.علاوه بر این ، این پاسخ در اشیاء «چتکومپتیکتولکال» تجزیه می شود ، که راحت است.

2. ** حالت JSON **: حالت JSON یک قابلیت انعطاف پذیر تر است که LLM را مجبور می کند همیشه یک رشته JSON معتبر را وارد کند ، اما ساختار JSON دلخواه است.وقتی به خروجی JSON نیاز دارید مفید است اما نمی خواهید ساختار دقیق را مشخص کنید.

به خاطر داشته باشید که LLM هنوز هم می تواند در هر دو رویکرد توهم کند.در تماس با عملکرد ، LLM ممکن است به جای استفاده از توابع ، دستورالعمل ها و متن آزاد را از بین ببرد ، یا ممکن است نام و مقادیر آرگومان را توهم کند.در حالت JSON ، LLM همیشه JSON تولید می کند ، اما قالب مشخص شده ممکن است رعایت نشود.

## نتیجه گیری

هر دو حالت JSON و فراخوان عملکرد ابزاری با ارزش برای به دست آوردن خروجی ساختاری از برنامه های LLM هستند.فراخوانی عملکرد کنترل بیشتری را فراهم می کند و در صورت امکان توصیه می شود ، در حالی که حالت JSON در صورت عدم اهمیت ساختار ، انعطاف پذیری را ارائه می دهد.با این حال ، اگر می خواهید کنترل بیشتری بر روی خروجی های خود داشته باشید ، [مربی] (https://github.com/daveebbelaar/python-openai-tutorial/tree/main/04٪20structured٪20output/instructor) راهی برای رفتن است.